{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324be65936cd4a40926530d1354023f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\deep_learning\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prk\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3afd7beb8549569428a74f5e865762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d497148a377c4fa0a962a15a6959cb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a868f35c75db4c2797332b0d21005530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c7fefb73f448dd82979b71533a6b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \"A journey of thousand miles begin with a single step.\"\n",
    "masked_sentence = \"A journey of thousand [MASK] begin with a single step.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1037, 4990, 1997, 4595,  103, 4088, 2007, 1037, 2309, 3357, 1012,\n",
       "          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = TOKENIZER(masked_sentence, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLMOutput(loss=None, logits=tensor([[[ -6.6964,  -6.6491,  -6.6678,  ...,  -6.0363,  -5.8445,  -4.0565],\n",
      "         [-17.3263, -17.2599, -17.3499,  ..., -14.9441, -15.1331,  -9.5796],\n",
      "         [-11.9478, -11.7128, -12.0379,  ...,  -8.9100, -10.4260,  -5.5944],\n",
      "         ...,\n",
      "         [ -9.4446, -10.2251,  -9.9707,  ...,  -7.7131,  -8.9814,  -7.0468],\n",
      "         [-13.0866, -12.7792, -13.1038,  ..., -11.0008, -10.5767,  -6.5901],\n",
      "         [-12.0449, -12.0285, -11.9642,  ..., -10.1097,  -9.6650,  -6.6063]]]), hidden_states=None, attentions=None)\n",
      "tensor([[[ -6.6964,  -6.6491,  -6.6678,  ...,  -6.0363,  -5.8445,  -4.0565],\n",
      "         [-17.3263, -17.2599, -17.3499,  ..., -14.9441, -15.1331,  -9.5796],\n",
      "         [-11.9478, -11.7128, -12.0379,  ...,  -8.9100, -10.4260,  -5.5944],\n",
      "         ...,\n",
      "         [ -9.4446, -10.2251,  -9.9707,  ...,  -7.7131,  -8.9814,  -7.0468],\n",
      "         [-13.0866, -12.7792, -13.1038,  ..., -11.0008, -10.5767,  -6.5901],\n",
      "         [-12.0449, -12.0285, -11.9642,  ..., -10.1097,  -9.6650,  -6.6063]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = MODEL(**inputs)\n",
    "    print(outputs)\n",
    "    predictions = outputs.logits\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Top Ids: [2086, 4084, 2097, 2661, 2420]\n",
      "Decoded Tokens: ['years', 'steps', 'will', 'miles', 'days']\n"
     ]
    }
   ],
   "source": [
    "mask_token_ids = (inputs.input_ids == TOKENIZER.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "GET_TOP_K = 5 #Ambil Prediksi 5 teratas\n",
    "predicted_top_ids = torch.topk(predictions[0, mask_token_ids], GET_TOP_K, dim=1).indices[0].tolist()\n",
    "print(\"Predicted Top Ids: \" + str(predicted_top_ids))\n",
    "\n",
    "decoded_tokens = [TOKENIZER.decode([token_id]) for token_id in predicted_top_ids] \n",
    "print(\"Decoded Tokens: \" + str(decoded_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3b3f27d57949558edbcbcafedfed06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\deep_learning\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prk\\.cache\\huggingface\\hub\\models--indobenchmark--indobert-base-p1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003c015e2b804127908b56e1d5f00f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57b46b75abf405db5d0f38a2d35d9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6384c4aa4b4f44adbbbf9e9903ee1bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5235be483e7c4f77ad79978ca7b18e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AutoModel \n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "MODEL = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \"Aku adalah anak tunggal.\"\n",
    "masked_sentence = \"Aku adalah anak [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,   304,   154,   436,     4, 30470,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = TOKENIZER(masked_sentence, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1615,  1.8124,  0.5361,  ...,  0.5347,  1.0343,  0.5585],\n",
      "         [-0.5848,  0.3835,  1.0228,  ..., -0.6247,  1.5687, -0.5214],\n",
      "         [-1.6725,  0.7773,  0.5695,  ...,  1.0071,  0.7778, -1.6788],\n",
      "         ...,\n",
      "         [-1.0949,  0.9401,  0.4040,  ...,  1.2156, -0.1618, -0.3143],\n",
      "         [-1.2305,  0.4968,  0.3990,  ...,  1.0511,  0.5109,  1.5673],\n",
      "         [-1.2982,  1.0834,  0.5484,  ...,  0.4890,  0.4464,  0.0541]]]), pooler_output=tensor([[-0.6814, -0.3680,  0.9994,  0.0442,  0.2172, -0.9825,  0.2126,  0.5749,\n",
      "          0.5274, -0.8885, -0.1847, -0.4516, -0.2283,  0.4196,  0.5083, -0.4174,\n",
      "          0.3209, -0.0306,  0.6425,  0.2136, -0.9672,  0.2230,  0.6849,  0.0031,\n",
      "         -0.1255,  0.4746,  0.6037,  0.9687, -0.1254,  0.1351,  0.8019, -0.8871,\n",
      "         -0.9950, -0.1820,  0.8168,  0.1906,  0.3705,  0.3598,  0.0664, -0.7297,\n",
      "          0.9947, -0.7108, -0.1339,  0.7235,  0.2981,  0.2638, -0.2369,  0.1464,\n",
      "          0.9618, -0.1504,  0.9267,  0.7568,  0.3141,  0.2717,  0.9980, -0.0666,\n",
      "          0.0216, -0.0483, -0.2297,  0.8153,  0.0950,  0.3606,  0.1268,  0.6623,\n",
      "         -0.9967, -0.7417, -0.8392, -0.6224,  0.1611,  0.1933, -0.8855,  0.0763,\n",
      "          0.5569,  0.3975, -0.3594,  0.4364, -0.1496, -0.9959,  0.5203,  0.4615,\n",
      "          0.4227,  0.4507, -0.2329, -0.9846, -0.8495,  0.1334, -0.5721,  0.0811,\n",
      "         -0.7848, -0.3813, -0.9763,  0.9985,  0.0684,  0.0943, -0.1107,  0.2251,\n",
      "          0.0950, -0.4633, -0.9406,  0.8025,  0.4570,  0.6008, -0.8958,  0.8498,\n",
      "          0.0537, -0.1380,  0.3224,  0.9658, -0.0135, -0.9881,  0.7593,  0.7134,\n",
      "          0.1075, -0.2686, -0.2139, -0.9981, -0.2151, -0.0660,  0.4415,  0.8777,\n",
      "          0.5708,  0.1431,  0.0258, -0.1114, -0.3100, -0.8458, -0.9906,  0.1026,\n",
      "         -0.0053, -0.1699, -0.7742,  0.8859,  0.7746,  0.6003,  0.0215,  0.2587,\n",
      "         -0.1405, -0.9782,  0.2740,  0.2475,  0.8925,  0.0772, -0.4426, -0.4697,\n",
      "         -0.9656,  0.0455, -0.1237,  0.3209,  0.9945,  0.3096, -0.6286, -0.0026,\n",
      "         -0.4007,  0.9764,  0.9614,  0.9723, -0.7379, -0.1967, -0.2092, -0.9545,\n",
      "          0.2268, -0.5232,  0.1190,  0.2199,  0.0284,  0.7243,  0.1151,  0.3760,\n",
      "          0.3984,  0.1067,  0.1005,  0.8452, -0.9105,  0.9973,  0.9687, -0.9767,\n",
      "         -0.9644,  0.9978,  0.9625, -0.9854,  0.8240,  0.8342, -0.8268, -0.9203,\n",
      "          0.0101, -0.0404, -0.9704,  0.2792,  0.3494, -0.9347, -0.4718,  0.8718,\n",
      "          0.1413,  0.8988,  0.4085,  0.2138,  0.1209,  0.6830, -0.9591,  0.1444,\n",
      "         -0.3108, -0.3705,  0.5853, -0.0103, -0.3256,  0.5556,  0.1605, -0.4502,\n",
      "          0.7595,  0.9697,  0.5699, -0.9857, -0.5970, -0.1198, -0.0578,  0.3155,\n",
      "          0.9797,  0.0705,  0.2117,  0.8874,  0.3775, -0.1782, -0.9288,  0.9698,\n",
      "         -0.3849, -0.0900, -0.9764,  0.9913,  0.4481, -0.2965,  0.3047, -0.2082,\n",
      "          0.8611, -0.6584,  0.1001,  0.1046, -0.7160, -0.0030,  0.0308, -0.6606,\n",
      "          0.0386,  0.9819, -0.7483, -0.0044, -0.0374,  0.7161, -0.8597,  0.2780,\n",
      "         -0.9647, -0.3577,  0.7465, -0.1351,  0.4572, -0.0985,  0.5971, -0.3023,\n",
      "         -0.2117,  0.9125,  0.3689,  0.9098,  0.3431, -0.9934, -0.4613,  0.6654,\n",
      "         -0.8666,  0.2938, -0.0048,  0.7308, -0.8064,  0.9388, -0.2744, -0.5573,\n",
      "         -0.0590,  0.1351, -0.6721,  0.9900,  0.9964,  0.7773,  0.9913, -0.1121,\n",
      "         -0.3937, -0.7556,  0.3707, -0.9019,  0.2708, -0.3372,  0.4443,  0.9990,\n",
      "         -0.9199,  0.2375, -0.8485,  0.9609, -0.1129, -0.5711, -0.1441, -0.4891,\n",
      "          0.0240, -0.1551,  0.9899, -0.2754,  0.9695, -0.3593, -0.1738,  0.4616,\n",
      "         -0.6001, -0.4537, -0.1300, -0.7933,  0.3213, -0.8394,  0.0243,  0.1347,\n",
      "         -0.9700, -0.9135, -0.0350,  0.3210,  0.9465,  0.6654,  0.5583, -0.3342,\n",
      "          0.3011, -0.0282, -0.4583, -0.3294,  0.1623, -0.1800, -0.4282,  0.9882,\n",
      "         -0.6614,  0.9551, -0.9984, -0.1055,  0.1629, -0.1924,  0.4594, -0.1932,\n",
      "          0.8366,  0.9837, -0.9954,  0.3477,  0.9884,  0.6515,  0.2311, -0.2631,\n",
      "          0.1605,  0.5738,  0.9844,  0.6094,  0.5459, -0.3283,  0.5769,  0.5986,\n",
      "         -0.9342,  0.1869,  0.1059,  0.1134,  0.5192,  0.9444,  0.8063, -0.0115,\n",
      "         -0.9408,  0.9975,  0.4265, -0.8890,  0.5838,  0.5931, -0.3039,  0.2588,\n",
      "         -0.0838, -0.7209, -0.0204,  0.3549, -0.1034, -0.3665, -0.3158, -0.9734,\n",
      "         -0.3262,  0.6578,  0.3805,  0.3299,  0.7295, -0.6690,  0.1199,  0.4190,\n",
      "         -0.0790,  0.3824,  0.1249,  0.5976,  0.0129, -0.8376, -0.3202, -0.9498,\n",
      "          0.6391, -0.0706, -0.9726, -0.0544,  0.2579,  0.9794, -0.9930,  0.1341,\n",
      "          0.5278, -0.8039, -0.9850, -0.1609,  0.3520,  0.3610,  0.2742,  0.2483,\n",
      "          0.9809,  0.4898, -0.9792, -0.2368, -0.5424, -0.5613, -0.0779,  0.3614,\n",
      "          0.9659,  0.3658, -0.7969,  0.1104,  0.9926,  0.1089,  0.7506,  0.3182,\n",
      "          0.2932, -0.6954,  0.5012, -0.8859,  0.6507,  0.4526,  0.2794,  0.9369,\n",
      "          0.1456,  0.6618,  0.1779,  0.8065,  0.6513,  0.2535, -0.2962, -0.0019,\n",
      "          0.2582,  0.8914,  0.6192,  0.7699, -0.7418, -0.9936, -0.8979, -0.9023,\n",
      "         -0.6401, -0.3774, -0.2507,  0.4140, -0.9999, -0.4852, -0.1648, -0.7512,\n",
      "         -0.9379,  0.9440,  0.9421, -0.1416,  0.1262,  0.0823, -0.4009, -0.3863,\n",
      "         -0.3456, -0.9093,  0.0148, -0.6919,  0.5251,  0.0479, -0.1267,  0.7772,\n",
      "          0.8936, -0.9081,  0.6626,  0.9214,  0.6647,  0.0308, -0.7259, -0.2714,\n",
      "         -0.8051,  0.9997,  0.2791,  0.8088, -0.0165,  0.6214,  0.2720,  0.9708,\n",
      "          0.6700, -0.1142, -0.0441, -0.6790,  0.5704, -0.7188, -0.2691, -0.1473,\n",
      "          0.9001,  0.8855,  0.9949,  0.0616, -0.2262,  0.4682, -0.1156, -0.1591,\n",
      "          0.9931,  0.0878,  0.2425,  0.2390,  0.9863, -0.9995,  0.9579, -0.0192,\n",
      "         -0.0855, -0.8174, -0.1505, -0.3495, -0.5935, -0.8750,  0.9942,  0.0516,\n",
      "         -0.0963, -0.1302,  0.1537,  0.9889, -0.9920,  0.0863, -0.9980, -0.9170,\n",
      "         -0.9087,  0.3198, -0.9963, -0.2678, -0.6488,  0.0682, -0.8441,  0.7002,\n",
      "          0.3504, -0.4275, -0.7704, -0.0070,  0.1374,  0.2174,  0.1555, -0.0196,\n",
      "         -0.7811, -0.8635, -0.9476, -0.3961,  0.0822, -0.9646, -0.5702,  0.9989,\n",
      "          0.9841,  0.7669,  0.9890,  0.8203, -0.9494,  0.2879, -0.1657, -0.9588,\n",
      "         -0.2289, -0.7244, -0.2679, -0.0265, -0.6182,  0.9334,  0.3991, -0.0533,\n",
      "         -0.9981,  0.8220, -0.3346, -0.0381, -0.0245, -0.1368, -0.3495,  0.7966,\n",
      "          0.0517, -0.9792, -0.7934, -0.5365, -0.9948,  0.2174,  0.5257,  0.9148,\n",
      "         -0.3472,  0.9555, -0.5061, -0.5940,  0.9987, -0.2251, -0.5096, -0.1230,\n",
      "          0.4226, -0.9925, -0.0505,  0.9054, -0.7813, -0.9722,  0.2493, -0.5902,\n",
      "         -0.7869, -0.1688, -0.7770,  0.2769,  0.4272,  0.9994,  0.9968,  0.0400,\n",
      "          0.4819, -0.3091, -0.6088,  0.8215,  0.7708,  0.1091,  0.1941, -0.3731,\n",
      "         -0.9992,  0.5465, -0.8105, -0.8445,  0.1668, -0.5213, -0.3105,  0.2666,\n",
      "         -0.0938, -0.1006,  0.5301,  0.9983, -0.1595, -0.0763, -0.8310,  0.0683,\n",
      "         -0.9700, -0.9343,  0.2229,  0.5624,  0.0104,  0.4183, -0.8654, -0.3633,\n",
      "         -0.9268, -0.5909,  0.0064,  0.6672,  0.0490, -0.0952, -0.4325, -0.0395,\n",
      "          0.5807, -0.9710, -0.9435,  0.1934, -0.4603,  0.9181,  0.4967, -0.4805,\n",
      "         -0.3437, -0.3916, -0.5078,  0.0689, -0.4745,  0.0987,  0.3839,  0.4090,\n",
      "          0.2192,  0.0430, -0.7127,  0.9968, -0.0210, -0.3370, -0.2468,  0.5266,\n",
      "          0.9114, -0.9999, -0.4652,  0.6479, -0.2780,  0.6134, -0.6679,  0.7437,\n",
      "          0.0415, -0.4403, -0.2053, -0.9490,  0.2973,  0.4011, -0.1786, -0.7649,\n",
      "         -0.0498,  0.0500, -0.9357,  0.5988, -0.5997, -0.4842,  0.2119, -0.7751,\n",
      "         -0.0709, -0.6475, -0.0852,  0.0429,  0.8961,  0.5832,  0.2205,  0.7736,\n",
      "          0.7044,  0.7483, -0.9872,  0.8084,  0.1239,  0.0864, -0.2292,  0.9148,\n",
      "         -0.9630, -0.9980,  0.7898, -0.4108,  0.1275, -0.9724,  0.1073,  0.9170,\n",
      "         -0.7773, -0.2804, -0.8927,  0.9794,  0.7676,  0.0066,  0.0796, -0.9337,\n",
      "         -0.3769, -0.8476,  0.9922, -0.3741, -0.0602, -0.0209, -0.1790,  0.7453,\n",
      "          0.2802, -0.7237, -0.4320, -0.1094, -0.0168,  0.1577, -0.1555, -0.6900,\n",
      "         -0.3596,  0.9962,  0.3593,  0.3843,  0.6705,  0.1761, -0.4538, -0.1151,\n",
      "          0.8746, -0.3415,  0.3349, -0.9999,  0.9563, -0.2492,  0.9142, -0.1029,\n",
      "         -0.8953,  0.5533, -0.4870, -0.4769, -0.1804, -0.1487, -0.4052,  0.9720]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m MODEL(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[1;32m----> 4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = MODEL(**inputs)\n",
    "    print(outputs)\n",
    "    predictions = outputs.logits\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Top Ids: [4595, 2116, 3634, 5190, 1996]\n",
      "Decoded Tokens: ['oke', 'aj', '##ancar', 'kesejahteraan', 'surabaya']\n"
     ]
    }
   ],
   "source": [
    "mask_token_ids = (inputs.input_ids == TOKENIZER.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "GET_TOP_K = 5 #Ambil Prediksi 5 teratas\n",
    "predicted_top_ids = torch.topk(predictions[0, mask_token_ids], GET_TOP_K, dim=1).indices[0].tolist()\n",
    "print(\"Predicted Top Ids: \" + str(predicted_top_ids))\n",
    "\n",
    "decoded_tokens = [TOKENIZER.decode([token_id]) for token_id in predicted_top_ids] \n",
    "print(\"Decoded Tokens: \" + str(decoded_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
